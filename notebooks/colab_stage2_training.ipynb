{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b979a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"header\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# üöÄ Sarika AI - Stage 2 Training\\n\",\n",
    "        \"## Context Teachers Sequential Distillation\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Author:** Noushad  \\n\",\n",
    "        \"**Repository:** [noushad999/sarika-ai](https://github.com/noushad999/sarika-ai)  \\n\",\n",
    "        \"**GPU Required:** T4 (16GB VRAM)  \\n\",\n",
    "        \"**Duration:** ~4-6 hours  \\n\",\n",
    "        \"\\n\",\n",
    "        \"### What This Does:\\n\",\n",
    "        \"- Loads Llama-3.1-8B student model\\n\",\n",
    "        \"- Trains with 6 specialized teacher models sequentially\\n\",\n",
    "        \"- Applies LoRA (only ~16M trainable parameters)\\n\",\n",
    "        \"- Saves checkpoints to Google Drive\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Teachers:\\n\",\n",
    "        \"1. **Bengali Culture** - Mistral-7B\\n\",\n",
    "        \"2. **Emotional Intelligence** - Qwen2.5-7B\\n\",\n",
    "        \"3. **Conversation Flow** - Llama-3.1-8B\\n\",\n",
    "        \"4. **Humor** - Gemma-2-9B\\n\",\n",
    "        \"5. **Deep Conversations** - Mistral-7B\\n\",\n",
    "        \"6. **Crisis Support** - Qwen2.5-7B\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"step1\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## ‚úÖ Step 1: Check GPU\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"check_gpu\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"!nvidia-smi\\n\",\n",
    "        \"\\n\",\n",
    "        \"import torch\\n\",\n",
    "        \"print(f\\\"\\\\n{'='*60}\\\")\\n\",\n",
    "        \"print(f\\\"PyTorch Version: {torch.__version__}\\\")\\n\",\n",
    "        \"print(f\\\"CUDA Available: {torch.cuda.is_available()}\\\")\\n\",\n",
    "        \"if torch.cuda.is_available():\\n\",\n",
    "        \"    print(f\\\"GPU: {torch.cuda.get_device_name(0)}\\\")\\n\",\n",
    "        \"    print(f\\\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\\\")\\n\",\n",
    "        \"print(f\\\"{'='*60}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"step2\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üìÇ Step 2: Mount Drive & Clone Repository\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"mount_drive\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Mount Google Drive\\n\",\n",
    "        \"from google.colab import drive\\n\",\n",
    "        \"drive.mount('/content/drive')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Navigate to Drive\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"os.chdir('/content/drive/MyDrive')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Clone repository (skip if already exists)\\n\",\n",
    "        \"if not os.path.exists('sarika-ai'):\\n\",\n",
    "        \"    !git clone https://github.com/noushad999/sarika-ai.git\\n\",\n",
    "        \"    print(\\\"‚úì Repository cloned\\\")\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    print(\\\"‚úì Repository already exists\\\")\\n\",\n",
    "        \"    %cd sarika-ai\\n\",\n",
    "        \"    !git pull origin main\\n\",\n",
    "        \"    print(\\\"‚úì Repository updated\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Change to project directory\\n\",\n",
    "        \"%cd sarika-ai\\n\",\n",
    "        \"\\n\",\n",
    "        \"!ls -la\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"step3\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üì¶ Step 3: Install Dependencies\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"install_deps\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Install required packages\\n\",\n",
    "        \"!pip install -q transformers accelerate peft bitsandbytes datasets huggingface_hub scipy\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"‚úì Dependencies installed\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"step4\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üîë Step 4: HuggingFace Login\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"hf_login\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Login to HuggingFace\\n\",\n",
    "        \"from huggingface_hub import login\\n\",\n",
    "        \"from getpass import getpass\\n\",\n",
    "        \"\\n\",\n",
    "        \"HF_TOKEN = getpass(\\\"Enter your HuggingFace token: \\\")\\n\",\n",
    "        \"login(token=HF_TOKEN)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Set environment variable\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"os.environ[\\\"HF_TOKEN\\\"] = HF_TOKEN\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"‚úì Logged in to HuggingFace\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"step5\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## ‚öôÔ∏è Step 5: Update Config for Colab\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"config\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"%%writefile ml/config.py\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"Sarika AI - Configuration (Colab Optimized)\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"import torch\\n\",\n",
    "        \"from pathlib import Path\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Paths - Colab\\n\",\n",
    "        \"PROJECT_ROOT = Path(\\\"/content/drive/MyDrive/sarika-ai\\\")\\n\",\n",
    "        \"CHECKPOINT_DIR = PROJECT_ROOT / \\\"ml\\\" / \\\"checkpoints\\\"\\n\",\n",
    "        \"MODEL_DIR = PROJECT_ROOT / \\\"models\\\"\\n\",\n",
    "        \"DATA_DIR = PROJECT_ROOT / \\\"data\\\"\\n\",\n",
    "        \"HF_HOME = str(PROJECT_ROOT / \\\"models\\\" / \\\"cache\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Create directories\\n\",\n",
    "        \"CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "        \"MODEL_DIR.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "        \"DATA_DIR.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Set environment\\n\",\n",
    "        \"os.environ[\\\"HF_HOME\\\"] = HF_HOME\\n\",\n",
    "        \"os.environ[\\\"TRANSFORMERS_CACHE\\\"] = HF_HOME\\n\",\n",
    "        \"os.environ[\\\"HF_DATASETS_CACHE\\\"] = str(DATA_DIR / \\\"datasets_cache\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Device\\n\",\n",
    "        \"DEVICE = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\",\n",
    "        \"GPU_MEMORY_GB = 16  # T4\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Tokens\\n\",\n",
    "        \"HF_TOKEN = os.getenv(\\\"HF_TOKEN\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Training Configuration\\n\",\n",
    "        \"class TrainingConfig:\\n\",\n",
    "        \"    # LoRA parameters\\n\",\n",
    "        \"    LORA_R = 16\\n\",\n",
    "        \"    LORA_ALPHA = 32\\n\",\n",
    "        \"    LORA_DROPOUT = 0.05\\n\",\n",
    "        \"    LORA_TARGET_MODULES = [\\n\",\n",
    "        \"        \\\"q_proj\\\", \\\"k_proj\\\", \\\"v_proj\\\", \\\"o_proj\\\",\\n\",\n",
    "        \"        \\\"gate_proj\\\", \\\"up_proj\\\", \\\"down_proj\\\"\\n\",\n",
    "        \"    ]\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Training hyperparameters\\n\",\n",
    "        \"    LEARNING_RATE = 2e-4\\n\",\n",
    "        \"    WEIGHT_DECAY = 0.01\\n\",\n",
    "        \"    MAX_GRAD_NORM = 1.0\\n\",\n",
    "        \"    MAX_SEQ_LENGTH = 512\\n\",\n",
    "        \"    BATCH_SIZE = 1\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Distillation\\n\",\n",
    "        \"    DISTILLATION_ALPHA = 0.5\\n\",\n",
    "        \"    TEMPERATURE = 2.0\\n\",\n",
    "        \"\\n\",\n",
    "        \"class LogConfig:\\n\",\n",
    "        \"    LOG_STEPS = 10\\n\",\n",
    "        \"    SAVE_STEPS = 100\\n\",\n",
    "        \"\\n\",\n",
    "        \"class SpaceConfig:\\n\",\n",
    "        \"    MAX_TOTAL_USAGE = 50\\n\",\n",
    "        \"    CLEANUP_THRESHOLD = 80\\n\",\n",
    "        \"    MAX_CHECKPOINTS = 3\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"‚úì Config loaded\\\")\\n\",\n",
    "        \"print(f\\\"  Device: {DEVICE}\\\")\\n\",\n",
    "        \"print(f\\\"  Project: {PROJECT_ROOT}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"step6\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üöÄ Step 6: Start Training!\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"train\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Run Stage 2 training\\n\",\n",
    "        \"!python ml/training/stage2_context_teachers.py\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"monitor\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üìä Monitor Progress\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"check_gpu_usage\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Check GPU usage\\n\",\n",
    "        \"!nvidia-smi\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"check_checkpoints\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# View checkpoints\\n\",\n",
    "        \"!ls -lh ml/checkpoints/stage2_context_teachers/\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"check_disk\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Check disk usage\\n\",\n",
    "        \"!df -h | grep drive\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"download\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üíæ Download Final Model (Optional)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"download_model\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Zip and download final model\\n\",\n",
    "        \"!cd ml/checkpoints/stage2_context_teachers && tar -czf final_model.tar.gz final_model/\\n\",\n",
    "        \"\\n\",\n",
    "        \"from google.colab import files\\n\",\n",
    "        \"files.download('ml/checkpoints/stage2_context_teachers/final_model.tar.gz')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"update\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üîÑ Update Code (If Needed)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"git_pull\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Pull latest changes from GitHub\\n\",\n",
    "        \"%cd /content/drive/MyDrive/sarika-ai\\n\",\n",
    "        \"!git pull origin main\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"troubleshoot\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## üõ†Ô∏è Troubleshooting\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Out of Memory?\\n\",\n",
    "        \"```python\\n\",\n",
    "        \"# Reduce batch size in config\\n\",\n",
    "        \"# Edit ml/config.py: BATCH_SIZE = 1\\n\",\n",
    "        \"```\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Session Timeout?\\n\",\n",
    "        \"```python\\n\",\n",
    "        \"# Training resumes from last checkpoint automatically\\n\",\n",
    "        \"# Just re-run the training cell\\n\",\n",
    "        \"```\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Model Download Failed?\\n\",\n",
    "        \"```python\\n\",\n",
    "        \"# Check HuggingFace token\\n\",\n",
    "        \"# Make sure you accepted Llama license: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\\n\",\n",
    "        \"```\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"accelerator\": \"GPU\",\n",
    "    \"colab\": {\n",
    "      \"gpuType\": \"T4\",\n",
    "      \"provenance\": [],\n",
    "      \"machine_shape\": \"hm\"\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
