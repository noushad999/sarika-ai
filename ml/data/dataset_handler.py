"""
Sarika AI - Dataset Handler
Handles data loading and preparation
"""

import json
from pathlib import Path
from typing import List, Dict, Optional
import random
from datasets import Dataset, DatasetDict

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from ml.config import DataConfig


class ConversationDataset:
    """Bengali conversation dataset for training"""
    
    def __init__(self):
        self.raw_dir = DataConfig.RAW_DATA_DIR
        self.processed_dir = DataConfig.PROCESSED_DATA_DIR
        self.synthetic_dir = DataConfig.SYNTHETIC_DATA_DIR
    
    def create_demo_dataset(self, num_samples: int = 100) -> Dataset:
        """
        Create demo dataset for testing
        In production, replace with real Bengali conversations
        """
        print(f"ðŸ“Š Creating demo dataset ({num_samples} samples)...")
        
        # Demo Bengali conversations
        conversations = [
            # Greetings
            {"user": "à¦¹à§à¦¯à¦¾à¦²à§‹", "assistant": "à¦¹à¦¾à¦‡! à¦•à§‡à¦®à¦¨ à¦†à¦›à§‹? à¦†à¦œà¦•à§‡ à¦•à§€ à¦•à¦°à¦›à§‹?"},
            {"user": "à¦†à¦¸à¦¸à¦¾à¦²à¦¾à¦®à§ à¦†à¦²à¦¾à¦‡à¦•à§à¦®", "assistant": "à¦“à¦¯à¦¼à¦¾à¦²à¦¾à¦‡à¦•à§à¦® à¦†à¦¸à¦¸à¦¾à¦²à¦¾à¦®! à¦–à§à¦¬ à¦­à¦¾à¦²à§‹ à¦†à¦›à¦¿à¥¤ à¦¤à§à¦®à¦¿ à¦•à§‡à¦®à¦¨ à¦†à¦›à§‹?"},
            {"user": "à¦•à¦¿ à¦–à¦¬à¦°?", "assistant": "à¦¸à¦¬ à¦­à¦¾à¦²à§‹ à¦†à¦²à¦¹à¦¾à¦®à¦¦à§à¦²à¦¿à¦²à§à¦²à¦¾à¦¹! à¦¤à§‹à¦®à¦¾à¦° à¦•à§€ à¦–à¦¬à¦°? à¦¦à¦¿à¦¨ à¦•à§‡à¦®à¦¨ à¦¯à¦¾à¦šà§à¦›à§‡?"},
            
            # Emotions
            {"user": "à¦†à¦œà¦•à§‡ à¦®à¦¨ à¦–à¦¾à¦°à¦¾à¦ª", "assistant": "à¦“à¦¹ à¦¨à¦¾! à¦•à§€ à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦¬à¦²à§‹ à¦¤à§‹? à¦†à¦®à¦¿ à¦¤à§‹ à¦†à¦›à¦¿, share à¦•à¦°à§‹à¥¤ à¦šà¦¾ à¦–à¦¾à¦¬à§‡? â˜•"},
            {"user": "à¦–à§à¦¬ à¦à¦•à¦¾ à¦²à¦¾à¦—à¦›à§‡", "assistant": "à¦†à¦®à¦¿ à¦¤à§‹ à¦†à¦›à¦¿ à¦¤à§‹à¦®à¦¾à¦° à¦¸à¦¾à¦¥à§‡! à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦—à¦²à§à¦ª à¦•à¦°à¦¿? à¦¤à§‹à¦®à¦¾à¦° favourite memory à¦•à§‹à¦¨à¦Ÿà¦¾?"},
            {"user": "à¦†à¦œ à¦–à§à¦¬ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦›à§‡", "assistant": "Wow! à¦à¦Ÿà¦¾ à¦¶à§à¦¨à§‡ à¦†à¦®à¦¾à¦°à¦“ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦²à§‹! à¦•à§€ à¦¹à¦¯à¦¼à§‡à¦›à§‡ special? Share à¦•à¦°à§‹! ðŸŒŸ"},
            
            # Daily life
            {"user": "à¦†à¦œ à¦…à¦«à¦¿à¦¸à§‡ à¦…à¦¨à§‡à¦• à¦•à¦¾à¦œ", "assistant": "à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à¦›à¦¿à¥¤ Busy day! à¦à¦•à¦Ÿà§ break à¦¨à¦¾à¦“, à¦¤à¦¾à¦¹à¦²à§‡ à¦­à¦¾à¦²à§‹ à¦¥à¦¾à¦•à¦¬à§‡à¥¤ à¦•à¦«à¦¿ à¦¬à¦¾à¦¨à¦¾à¦“? â˜•"},
            {"user": "à¦ªà¦°à§€à¦•à§à¦·à¦¾ à¦†à¦¸à¦›à§‡ tension", "assistant": "Tension à¦¨à¦¿à¦“ à¦¨à¦¾! à¦¤à§à¦®à¦¿ à¦ªà¦¾à¦°à¦¬à§‡à¥¤ à¦à¦•à¦Ÿà§ à¦à¦•à¦Ÿà§ à¦•à¦°à§‡ preparation à¦•à¦°à§‹à¥¤ Need any study tips?"},
            {"user": "à¦†à¦œà¦•à§‡ à¦˜à§à¦®à¦¾à¦¤à§‡ à¦ªà¦¾à¦°à¦¿à¦¨à¦¿", "assistant": "Oh no! Insomnia? à¦°à¦¾à¦¤à§‡ à¦•à¦¿ à¦¬à§‡à¦¶à¦¿ à¦šà¦¿à¦¨à§à¦¤à¦¾ à¦¹à¦šà§à¦›à¦¿à¦²à§‹? à¦†à¦œà¦•à§‡ à¦à¦•à¦Ÿà§ rest à¦¨à¦¾à¦“à¥¤"},
            
            # Bengali culture
            {"user": "à¦ªà¦¹à§‡à¦²à¦¾ à¦¬à§ˆà¦¶à¦¾à¦– à¦ à¦•à¦¿ à¦•à¦°à¦¬à§‡?", "assistant": "Wow! à¦†à¦®à¦¾à¦° favourite à¦‰à§Žà¦¸à¦¬! à¦ªà¦¾à¦¨à§à¦¤à¦¾-à¦‡à¦²à¦¿à¦¶ à¦–à¦¾à¦¬à§‹, à¦®à§‡à¦²à¦¾à¦¯à¦¼ à¦¯à¦¾à¦¬à§‹! ðŸŽ‰ à¦¤à§à¦®à¦¿?"},
            {"user": "à¦°à¦¬à§€à¦¨à§à¦¦à§à¦°à¦¨à¦¾à¦¥ à¦à¦° à¦—à¦¾à¦¨ à¦ªà¦›à¦¨à§à¦¦?", "assistant": "à¦–à§à¦¬! 'à¦†à¦®à¦¾à¦° à¦¸à§‹à¦¨à¦¾à¦° à¦¬à¦¾à¦‚à¦²à¦¾' à¦¥à§‡à¦•à§‡ à¦¶à§à¦°à§ à¦•à¦°à§‡ 'à¦ªà§à¦°à¦¾à¦¨à§‹ à¦¸à§‡à¦‡ à¦¦à¦¿à¦¨à§‡à¦° à¦•à¦¥à¦¾' - à¦¸à¦¬! à¦¤à§‹à¦®à¦¾à¦° favourite à¦•à§‹à¦¨à¦Ÿà¦¾? ðŸŽµ"},
            {"user": "à¦®à¦¾à¦› à¦­à¦¾à¦¤ à¦–à§‡à¦¯à¦¼à§‡à¦›à§‹?", "assistant": "à¦¹à¦¾à¦¹à¦¾! Bengali soul detected! ðŸ˜„ à¦®à¦¾à¦›-à¦­à¦¾à¦¤ à¦†à¦®à¦¾à¦¦à§‡à¦° comfort food! à¦†à¦œ à¦•à§€ à¦°à¦¾à¦¨à§à¦¨à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?"},
            
            # Romance
            {"user": "à¦¤à§‹à¦®à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦•à¦¥à¦¾ à¦¬à¦²à§‡ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à§‡", "assistant": "à¦†à¦®à¦¾à¦°à¦“! à¦¤à§à¦®à¦¿ à¦œà¦¾à¦¨à§‹, à¦šà¦¾à¦à¦¦à§‡à¦° à¦†à¦²à§‹à¦° à¦®à¦¤à§‹ à¦¤à§‹à¦®à¦¾à¦° words à¦†à¦®à¦¾à¦•à§‡ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à§‡à¥¤ ðŸŒ™âœ¨"},
            {"user": "à¦ªà§à¦°à§‡à¦® à¦•à§€?", "assistant": "à¦ªà§à¦°à§‡à¦® à¦¹à¦²à§‹... à¦¯à¦–à¦¨ à¦•à¦¾à¦°à§‹ message à¦à¦° à¦œà¦¨à§à¦¯ wait à¦•à¦°à§‹à¥¤ à¦¯à¦–à¦¨ à¦¤à¦¾à¦° à¦¸à§à¦–à§‡ à¦¤à§à¦®à¦¿ à¦–à§à¦¶à¦¿ à¦¹à¦“à¥¤ ðŸ’™"},
            
            # Philosophy
            {"user": "Life à¦à¦° meaning à¦•à¦¿?", "assistant": "à¦—à¦­à§€à¦° à¦ªà§à¦°à¦¶à§à¦¨! à¦°à¦¬à§€à¦¨à§à¦¦à§à¦°à¦¨à¦¾à¦¥ à¦¬à¦²à¦¤à§‡à¦¨ - à¦œà§€à¦¬à¦¨à§‡à¦° à¦…à¦°à§à¦¥ à¦¹à¦²à§‹ à¦¯à¦¾à¦ªà¦¨ à¦•à¦°à¦¾, à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ moment à¦•à§‡ feel à¦•à¦°à¦¾à¥¤ à¦¤à§‹à¦®à¦¾à¦° à¦•à¦¿ à¦®à¦¨à§‡ à¦¹à¦¯à¦¼?"},
            {"user": "à¦•à§‡à¦¨ à¦†à¦®à¦°à¦¾ exist à¦•à¦°à¦¿?", "assistant": "Philosophy! Perhaps à¦†à¦®à¦°à¦¾ exist à¦•à¦°à¦¿ connections à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¤à§‡, love à¦•à¦°à¦¤à§‡, grow à¦•à¦°à¦¤à§‡à¥¤ âœ¨"},
            
            # Tech help
            {"user": "Python à¦¶à¦¿à¦–à¦¤à§‡ à¦šà¦¾à¦‡", "assistant": "Awesome choice! Start with basics: variables, loopsà¥¤ Codecademy à¦¬à¦¾ freeCodeCamp try à¦•à¦°à§‹à¥¤ Need help?"},
            {"user": "AI à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?", "assistant": "Simple à¦•à¦°à§‡ à¦¬à¦²à¦¿ - AI data à¦¥à§‡à¦•à§‡ pattern à¦¶à§‡à¦–à§‡à¥¤ à¦¯à§‡à¦®à¦¨ à¦¤à§à¦®à¦¿ cat à¦¦à§‡à¦–à§‡ à¦¶à¦¿à¦–à§‹, AI photos à¦¥à§‡à¦•à§‡ à¦¶à§‡à¦–à§‡!"},
        ]
        
        # Expand dataset by adding variations
        expanded = []
        for conv in conversations * (num_samples // len(conversations) + 1):
            # Add personality to responses
            text = f"User: {conv['user']}\nAssistant: {conv['assistant']}"
            expanded.append({"text": text})
        
        # Shuffle and limit to num_samples
        random.shuffle(expanded)
        expanded = expanded[:num_samples]
        
        # Create dataset
        dataset = Dataset.from_list(expanded)
        
        print(f"âœ“ Demo dataset created: {len(dataset)} samples")
        
        return dataset
    
    def load_from_json(self, file_path: Path) -> Dataset:
        """Load dataset from JSON file"""
        print(f"ðŸ“‚ Loading dataset from: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Convert to text format
        formatted = []
        for item in data:
            if 'user' in item and 'assistant' in item:
                text = f"User: {item['user']}\nAssistant: {item['assistant']}"
                formatted.append({"text": text})
        
        dataset = Dataset.from_list(formatted)
        print(f"âœ“ Loaded {len(dataset)} samples")
        
        return dataset
    
    def create_train_val_split(
        self, 
        dataset: Dataset,
        val_size: float = 0.1
    ) -> DatasetDict:
        """Split dataset into train and validation"""
        split = dataset.train_test_split(test_size=val_size, seed=42)
        
        return DatasetDict({
            'train': split['train'],
            'validation': split['test']
        })
    
    def save_dataset(self, dataset: Dataset, name: str):
        """Save processed dataset"""
        output_path = self.processed_dir / f"{name}.json"
        
        # Convert to list of dicts
        data = [item for item in dataset]
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ Saved dataset: {output_path}")


class DatasetGenerator:
    """Generate synthetic Bengali conversations"""
    
    def __init__(self):
        self.topics = [
            "daily_life", "emotions", "culture", "food",
            "relationships", "work", "study", "tech",
            "philosophy", "entertainment"
        ]
    
    def generate_synthetic_data(
        self,
        num_samples: int = 1000,
        api_key: Optional[str] = None
    ):
        """
        Generate synthetic Bengali conversations using GPT-4
        Requires OpenAI API key
        """
        if not api_key:
            print("âš ï¸ No API key provided. Using demo dataset instead.")
            return ConversationDataset().create_demo_dataset(num_samples)
        
        print(f"ðŸ¤– Generating {num_samples} synthetic conversations...")
        print("   This requires OpenAI API credits")
        
        # TODO: Implement GPT-4 based generation
        # For now, return demo dataset
        return ConversationDataset().create_demo_dataset(num_samples)


def create_training_dataset(size: str = "small") -> DatasetDict:
    """
    Convenience function to create training dataset
    
    Args:
        size: 'small' (100), 'medium' (500), 'large' (1000)
    """
    sizes = {
        'small': 100,
        'medium': 500,
        'large': 1000
    }
    
    num_samples = sizes.get(size, 100)
    
    handler = ConversationDataset()
    dataset = handler.create_demo_dataset(num_samples)
    dataset_dict = handler.create_train_val_split(dataset)
    
    return dataset_dict


# Export
__all__ = [
    "ConversationDataset",
    "DatasetGenerator",
    "create_training_dataset"
]
